{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "127a0933",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing dependencies\n",
    "import numpy\n",
    "import sys\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31b9ce04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading data\n",
    "with open('frankenstein.txt',encoding='utf8') as f:\n",
    "    data=f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b1941fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenization and standardization\n",
    "def tokenize_words(input):\n",
    "    # lowercase everything to standardize it\n",
    "    input = input.lower()\n",
    "\n",
    "    # instantiate the tokenizer\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(input)\n",
    "\n",
    "    # if the created token isn't in the stop words, make it part of \"filtered\"\n",
    "    filtered = filter(lambda token: token not in stopwords.words('english'), tokens)\n",
    "    return \" \".join(filtered)\n",
    "\n",
    "processed_inputs=tokenize_words(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c2972063",
   "metadata": {},
   "outputs": [],
   "source": [
    "#characters to numbers\n",
    "#we'll sort the list pf set of all characters that appear in out i/p text and then use enumerate fn to get numbers that represent\n",
    "#the characters\n",
    "chars = sorted(list(set(processed_inputs)))\n",
    "char_to_num = dict((c, i) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7adff3d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters:  269878\n",
      "Total vocabulary:  43\n"
     ]
    }
   ],
   "source": [
    "#checking if words to characters or characters to number has worked\n",
    "input_len=len(processed_inputs)\n",
    "vocab_len=len(chars)\n",
    "print(\"Total number of characters: \", input_len)\n",
    "print(\"Total vocabulary: \", vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd18ca10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sequence length\n",
    "#defining the length of an individual sequence\n",
    "#a sequence is a complete mapping of input characters as integers\n",
    "seq_length=100\n",
    "x_data=[]\n",
    "y_data=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2ac25860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  539556\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# loop through inputs, start at the beginning and go until we hit\n",
    "# the final character we can create a sequence out of\n",
    "for i in range(0, input_len - seq_length, 1):\n",
    "    # Define input and output sequences\n",
    "    # Input is the current character plus desired sequence length\n",
    "    in_seq = processed_inputs[i:i + seq_length]\n",
    "\n",
    "    # Out sequence is the initial character plus total sequence length\n",
    "    out_seq = processed_inputs[i + seq_length]\n",
    "\n",
    "    # We now convert list of characters to integers based on\n",
    "    # previously and add the values to our lists\n",
    "    x_data.append([char_to_num[char] for char in in_seq])\n",
    "    y_data.append(char_to_num[out_seq])\n",
    "    \n",
    "n_patterns=len(x_data)\n",
    "print(\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a1a2e199",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting sequence into np array\n",
    "X = numpy.reshape(x_data, (n_patterns, seq_length, 1))\n",
    "X = X/float(vocab_len)\n",
    "\n",
    "#one-hot encoding\n",
    "y=np_utils.to_categorical(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6957b343",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "\n",
    "#compiling the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "457d4c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving weights\n",
    "filepath = \"model_weights_saved.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "desired_callbacks = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3c4b518c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "2108/2108 [==============================] - 6259s 3s/step - loss: 2.8833\n",
      "\n",
      "Epoch 00001: loss improved from inf to 2.73049, saving model to model_weights_saved.hdf5\n",
      "Epoch 2/4\n",
      "2108/2108 [==============================] - 6237s 3s/step - loss: 2.4016\n",
      "\n",
      "Epoch 00002: loss improved from 2.73049 to 2.33527, saving model to model_weights_saved.hdf5\n",
      "Epoch 3/4\n",
      "2108/2108 [==============================] - 9106s 4s/step - loss: 2.1719\n",
      "\n",
      "Epoch 00003: loss improved from 2.33527 to 2.13399, saving model to model_weights_saved.hdf5\n",
      "Epoch 4/4\n",
      "2108/2108 [==============================] - 6550s 3s/step - loss: 2.0330\n",
      "\n",
      "Epoch 00004: loss improved from 2.13399 to 2.01200, saving model to model_weights_saved.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e9946e6100>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit the model and training it\n",
    "model.fit(X, y, epochs=4, batch_size=256, callbacks=desired_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b367c92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"model_weights_saved.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "69fa6262",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0c7184e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:\n",
      "\" window figure hideous abhorred grin face monster seemed jeer fiendish finger pointed towards corpse  \"\n"
     ]
    }
   ],
   "source": [
    "start = numpy.random.randint(0, len(x_data) - 1)\n",
    "pattern = x_data[start]\n",
    "print(\"Random Seed:\")\n",
    "print(\"\\\"\", ''.join([num_to_char[value] for value in pattern]), \"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ad6720cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sears sear"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(vocab_len)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = num_to_char[index]\n",
    "    \n",
    "    seq_in=[num_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf436f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
